{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GiL_39bYC6dT",
        "3l9R8gV2C_ZJ",
        "Ywk9QTWzEOtc",
        "nq6619RPJBFb",
        "ZYI_XlxDJJU4",
        "za-H_n-3JRBd"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LUNAR LANDER\n",
        "\n",
        "El objetivo del juego es simple (¬°pero aterrizar no lo es!): ¬°aterrizar la nave espacial sana y salva en la plataforma designada! ¬°Prep√°rate para un aterrizaje suave y heroico! üöÄüåï\n"
      ],
      "metadata": {
        "id": "hmWbGiyvNNME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reglas y Punteo\n",
        "En cada momento del juego, ganas o pierdes puntos (recompensa) dependiendo de c√≥mo te vaya:\n",
        "\n",
        "**Aterrizaje y velocidad**: Ganas puntos si te acercas a la zona de aterrizaje y vas despacio. Pierdes puntos si te alejas o vas muy r√°pido.\n",
        "\n",
        "**Inclinaci√≥n**: Pierdes puntos si la nave est√° muy inclinada. ¬°Tienes que mantenerla lo m√°s horizontal posible!\n",
        "\n",
        "**Patas en el suelo**: Ganas **10** puntos por cada pata que toca el suelo en la zona de aterrizaje.\n",
        "\n",
        "**Motores**: Pierdes puntos por usar los motores: un poquito por los motores laterales y m√°s por el motor principal. ¬°Hay que usarlos con cuidado!\n",
        "\n",
        "**Final del juego**: Si te estrellas, pierdes **100** puntos. Si aterrizas suavemente en la plataforma, ¬°ganas **100** puntos extra!\n",
        "\n",
        "Para considerar que has tenido √©xito en un intento (episodio), ¬°necesitas conseguir al menos **200** puntos en total!"
      ],
      "metadata": {
        "id": "OvS3IEo_Pmv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalacion de librerias"
      ],
      "metadata": {
        "id": "GiL_39bYC6dT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isgdPYBPWrSE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Permite conectar codigo en C, C++ con Python\n",
        "# Requerido por box2d\n",
        "!pip install -q swig\n",
        "\n",
        "# Gymnasium provee entornos de simulacion, controles y califica resultado\n",
        "!pip install -q \"gymnasium[classic-control]\"\n",
        "!pip install -q gymnasium[box2d]\n",
        "\n",
        "# Para grabar y reproducir video\n",
        "# !pip install moviepy\n",
        "!pip install -q pyvirtualdisplay\n",
        "\n",
        "# implementaciones de RL, DQN (Deep Q-Learning)\n",
        "!pip install -q stable-baselines3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variables globales"
      ],
      "metadata": {
        "id": "3l9R8gV2C_ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ENV_NAME = \"LunarLander-v3\" # Nombre del entorno\n",
        "VIDEO_FOLDER = \"./video_prueba_de_vuelo\" # En esta carpeta se guardaran los videos del test de vuelo\n",
        "EPISODES = 1 # Numero de episodios a grabar en la prueba de vuelo, se tratara de seleccionar el mejor\n",
        "LOG_DIR = \"./tmp/dqn_lunar\" # Carpeta donde se guardar√°n los registros de entrenamiento (logs)"
      ],
      "metadata": {
        "id": "502iTO5rCz_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenando el modelo"
      ],
      "metadata": {
        "id": "Ywk9QTWzEOtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# ENTRENAMIENTO DE UN AGENTE DQN (Stable-Baselines3)\n",
        "# ==============================================================================\n",
        "\n",
        "# Gymnasium provee el entorno, controles y evalua el resultado\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import os\n",
        "# import moviepy.editor as mp # Importamos MoviePy\n",
        "\n",
        "\n",
        "# Agente DQN, al que entrenaremos\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "\n",
        "# --- Preparaci√≥n para el entrenamiento ---\n",
        "# La grabaci√≥n de video solo debe hacerse despu√©s del entrenamiento o en un ambiente separado.\n",
        "# Para entrenar, usaremos una versi√≥n simple del ambiente sin el wrapper de video.\n",
        "\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "\n",
        "# Crear el ambiente para el entrenamiento (usando Monitor para guardar logs)\n",
        "env_train = gym.make(\n",
        "    ENV_NAME,\n",
        "    continuous=False,\n",
        "    gravity=-10,\n",
        "    enable_wind=False,\n",
        "    wind_power=15.0,\n",
        "    turbulence_power=1.5\n",
        ")\n",
        "env_train = Monitor(env_train, LOG_DIR)\n",
        "\n",
        "# Stable-Baselines3 funciona mejor con entornos vectorizados\n",
        "env_train_vec = make_vec_env(lambda: env_train, n_envs=1)\n",
        "\n",
        "\n",
        "# --- Creaci√≥n del Modelo DQN (Deep Q-Network) ---\n",
        "# DQN es un algoritmo de Aprendizaje por Refuerzo que combina Q-Learning con Redes Neuronales Profundas (Deep Learning).\n",
        "# Es la elecci√≥n ideal para entornos donde las acciones son discretas (un n√∫mero limitado de opciones), como el LunarLander-v3.\n",
        "\n",
        "model = DQN(\n",
        "    \"MlpPolicy\",             # ‚û°Ô∏è Policy (Pol√≠tica) del Agente: Define la arquitectura de la Red Neuronal (NN).\n",
        "                             #    \"MlpPolicy\" (Multi-Layer Perceptron) usa una red simple (Redes DENSAS) para mapear\n",
        "                             #    el estado (ej. posici√≥n del Lander) a los Q-valores de las acciones posibles.\n",
        "\n",
        "    env_train_vec,           # ‚û°Ô∏è Entorno de Entrenamiento (Vectorizado): El 'mundo' donde el agente aprende.\n",
        "                             #    Debe estar vectorizado para un entrenamiento m√°s r√°pido y eficiente.\n",
        "\n",
        "    learning_rate=0.0001,    # ‚û°Ô∏è Tasa de Aprendizaje (Alpha): Controla qu√© tan r√°pido la NN ajusta sus pesos\n",
        "                             #    en cada paso de entrenamiento. Un valor menor (ej. 0.00001) hace el aprendizaje m√°s lento\n",
        "                             #    pero m√°s estable; uno mayor (ej. 0.001) lo acelera, pero corre riesgo de no converger.\n",
        "\n",
        "    buffer_size=10000,       # ‚û°Ô∏è Tama√±o del 'Experience Replay Buffer': Es el \"ba√∫l de los recuerdos\" del agente.\n",
        "                             #    Define cu√°ntas experiencias (transiciones: estado, acci√≥n, recompensa, nuevo_estado)\n",
        "                             #    puede guardar el agente para re-usarlas en el entrenamiento (Experience Replay).\n",
        "\n",
        "    learning_starts=5000,    # ‚û°Ô∏è Cu√°ndo Iniciar el Aprendizaje: N√∫mero de timesteps (interacciones) que debe acumular\n",
        "                             #    el agente en su 'buffer' antes de empezar a entrenarse con ellas.\n",
        "\n",
        "    batch_size=64,           # ‚û°Ô∏è Tama√±o del Lote (Batch Size): N√∫mero de experiencias tomadas al azar del 'buffer'\n",
        "                             #    en cada paso de entrenamiento de la Red Neuronal. Valores comunes son 32, 64 o 128.\n",
        "\n",
        "    gamma=0.99,              # ‚û°Ô∏è Factor de Descuento (Discount Factor): El peso que le damos a las recompensas futuras.\n",
        "                             #    - Un valor menor (ej. 0.90) hace que el agente busque recompensas inmediatas.\n",
        "                             #    - Un valor mayor (ej. 0.99) hace que el agente sea m√°s \"cuidadoso\", valorando m√°s las\n",
        "                             #    recompensas que vendr√°n a largo plazo (es esencial para tareas secuenciales).\n",
        "\n",
        "    verbose=1,               # ‚û°Ô∏è Nivel de Verbosidad: 0 (silencioso), 1 (mostrar progreso), 2 (mostrar mucha m√°s info de debug).\n",
        "\n",
        "    tensorboard_log=LOG_DIR  # ‚û°Ô∏è Directorio para logs: Ubicaci√≥n donde se guardan los datos para monitorear\n",
        "                             #    el entrenamiento con TensorBoard.\n",
        ")\n",
        "\n",
        "# --- Bucle de Aprendizaje ---\n",
        "# El m√©todo .learn() es el n√∫cleo del entrenamiento de RL.\n",
        "# Ejecuta la interacci√≥n del agente con el entorno, recolectando experiencias (Experience Replay) y entrenando\n",
        "# la red neuronal (Deep Q-Network).\n",
        "\n",
        "TIMESTEPS = 100_000 # ‚û°Ô∏è Duraci√≥n del Entrenamiento (Timesteps Totales)\n",
        "                     # Este es el n√∫mero total de pasos de interacci√≥n que el agente realizar√° en el entorno\n",
        "                     # ANTES de detener el entrenamiento. Se entrena por (50,000 - 200,000) pasos en este caso.\n",
        "\n",
        "# üí° Diferencia clave:\n",
        "# TIMESTEP (Paso de tiempo): UNA SOLA interacci√≥n: (Agente hace Acci√≥n -> Entorno da Recompensa y nuevo Estado).\n",
        "# EPISODIO: Una secuencia completa de Timesteps que termina en √©xito o fracaso (ej. el Lander aterriza o choca).\n",
        "\n",
        "# Entrenaremos por 100,000 timesteps. Esto tomar√° unos minutos en Colab.\n",
        "model.learn(\n",
        "    total_timesteps=TIMESTEPS,\n",
        "    log_interval=4,\n",
        "    tb_log_name=\"DQN_Custom\"\n",
        ")\n",
        "print(f\"\\n--- INICIANDO ENTRENAMIENTO DQN por {TIMESTEPS} pasos ---\")\n",
        "\n",
        "# Entrenar!!\n",
        "model.learn(\n",
        "    total_timesteps=TIMESTEPS,\n",
        "    log_interval=100\n",
        ")\n",
        "\n",
        "print(\"\\n--- ENTRENAMIENTO FINALIZADO. Modelo entrenado guardado. ---\")\n",
        "model.save(\"modelo_nave_entrenada\") # Guarda el modelo entrenado\n",
        "env_train.close()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OtROtkf7gzka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prueba de Vuelo"
      ],
      "metadata": {
        "id": "nq6619RPJBFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 4. PRUEBA DE VUELO Y GRABAR EL VIDEO\n",
        "# ==============================================================================\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import glob\n",
        "import io\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "# Google collab tiene dependencias core deprecadas\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 1. Configurar la Pantalla Virtual (Necesario para Colab/Jupyter sin GUI)\n",
        "print(\"\\n--- Configurando Pantalla Virtual ---\")\n",
        "try:\n",
        "    display = Display(visible=0, size=(640, 480))\n",
        "    display.start()\n",
        "    print(\"Pantalla virtual iniciada.\")\n",
        "except Exception as e:\n",
        "    print(f\"Advertencia al iniciar pyvirtualdisplay: {e}. Continuaremos.\")\n",
        "\n",
        "# 2. Crear un nuevo ambiente con el wrapper RecordVideo\n",
        "# Creamos la carpeta de video si no existe\n",
        "os.makedirs(VIDEO_FOLDER, exist_ok=True)\n",
        "print(f\"Grabando {EPISODES} episodio(s) en la carpeta: {VIDEO_FOLDER}\")\n",
        "\n",
        "# Creamos el ambiente de test con el wrapper de video\n",
        "env_test = gym.make(\n",
        "    ENV_NAME,\n",
        "    continuous=False,\n",
        "    gravity=-10,\n",
        "    enable_wind=False,\n",
        "    wind_power=15.0,\n",
        "    turbulence_power=0.1,\n",
        "    render_mode=\"rgb_array\"\n",
        ")\n",
        "# El wrapper de RecordVideo debe ser el que envuelve al ambiente base\n",
        "env_test_video = RecordVideo(\n",
        "    env_test,\n",
        "    video_folder=VIDEO_FOLDER,\n",
        "    episode_trigger=lambda x: x == 0, # Graba solo el primer episodio\n",
        "    name_prefix=\"prueba_de_vuelo\"\n",
        ")\n",
        "\n",
        "# 3. Cargar el modelo entrenado y ejecutar un episodio\n",
        "# Cargamos el modelo que acabamos de entrenar y guardar\n",
        "model = DQN.load(\"modelo_nave_entrenada\", env=env_test_video)\n",
        "\n",
        "obs, info = env_test_video.reset()\n",
        "done = False\n",
        "truncated = False\n",
        "while not (done or truncated):\n",
        "    # El modelo determina la acci√≥n\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    # Ejecutamos la acci√≥n\n",
        "    obs, reward, done, truncated, info = env_test_video.step(action)\n",
        "\n",
        "env_test_video.close()\n",
        "print(\"\\n--- Grabaci√≥n del video finalizada. ---\")"
      ],
      "metadata": {
        "id": "jL7FmdKqhZaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reproducir Video de la prueba"
      ],
      "metadata": {
        "id": "ZYI_XlxDJJU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 5. CARGAR Y REPRODUCIR EL VIDEO DE LA PRUEBA DE VUELO\n",
        "# ==============================================================================\n",
        "import os\n",
        "import glob\n",
        "import io\n",
        "from IPython.display import HTML, display\n",
        "from base64 import b64encode\n",
        "\n",
        "# 1. Funci√≥n para codificar y mostrar un video usando Base64\n",
        "def display_encoded_video(video_path):\n",
        "    \"\"\"Codifica un video a Base64 y lo muestra en un Jupyter/Colab notebook.\"\"\"\n",
        "    print(f\"Mostrando: {video_path}\")\n",
        "\n",
        "    try:\n",
        "        # Abrir y codificar el archivo\n",
        "        with io.open(video_path, 'rb') as f:\n",
        "            video_bytes = f.read()\n",
        "        video_encoded = b64encode(video_bytes).decode()\n",
        "\n",
        "        # Crear y mostrar el tag de video HTML\n",
        "        html_tag = f\"\"\"\n",
        "        <video width=\"600\" controls autoplay>\n",
        "            <source src=\"data:video/mp4;base64,{video_encoded}\" type=\"video/mp4\">\n",
        "            Tu navegador no soporta el tag de video.\n",
        "        </video>\n",
        "        <p>--------------------------------------------------</p>\n",
        "        \"\"\"\n",
        "        display(HTML(html_tag))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERROR al procesar o mostrar el video {video_path}: {e}\")\n",
        "        print(\"Esto podr√≠a ser por un archivo muy grande.\")\n",
        "\n",
        "\n",
        "# 2. Buscar todos los archivos .mp4 en la carpeta\n",
        "# Ordenamos por fecha de creaci√≥n para verlos en orden de grabaci√≥n\n",
        "list_of_files = sorted(\n",
        "    glob.glob(os.path.join(VIDEO_FOLDER, \"*.mp4\")),\n",
        "    key=os.path.getctime\n",
        ")\n",
        "\n",
        "# 3. Iterar y mostrar cada video\n",
        "if list_of_files:\n",
        "    print(f\"‚úÖ Se encontraron {len(list_of_files)} videos para reproducir.\")\n",
        "    for video_file in list_of_files:\n",
        "        display_encoded_video(video_file)\n",
        "else:\n",
        "    print(f\"‚ùå No se encontr√≥ ning√∫n archivo de video MP4 en {VIDEO_FOLDER}.\")"
      ],
      "metadata": {
        "id": "VAqKRsjN2gb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Puntaje de la prueba"
      ],
      "metadata": {
        "id": "za-H_n-3JRBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------\n",
        "# CALIFICACION DEL ENTRENAMIENTO\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# Asume que estas variables ya han sido actualizadas por env_test_video.step()\n",
        "# reward, done, truncated, info\n",
        "\n",
        "\n",
        "# Imprimir cada variable en una l√≠nea separada\n",
        "print(f\"Reward (Recompensa): {reward:.2f}\")\n",
        "print(f\"Done (Logro Completar?): {done}\")\n",
        "print(f\"Truncated (Tuvo que interrumpirse?): {truncated}\")\n",
        "print(f\"Info (Informaci√≥n): {info}\")"
      ],
      "metadata": {
        "id": "uabPapxG2_nO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}